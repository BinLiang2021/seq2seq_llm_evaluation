# seq2seq_llm_evaluation
This repository contains the code used to produce the results for the paper "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks", published at EMNLP 2023.
